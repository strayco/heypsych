#!/usr/bin/env tsx
/**
 * JSON ‚Üí Database Sync Script
 *
 * Purpose: Synchronizes JSON content files to Supabase database
 * JSON remains the canonical source of truth
 * Database acts as runtime mirror for performance
 *
 * Usage:
 *   npm run sync:content              # Full sync
 *   npm run sync:content -- --dry-run # Preview changes
 *   npm run sync:content -- --type=treatments # Sync specific type
 *
 * Workflow:
 *   1. Read JSON files recursively
 *   2. Validate against schemas
 *   3. Normalize to Entity format
 *   4. Batch upsert to database
 *   5. Report statistics
 */

import fs from "fs";
import path from "path";
import { createClient } from "@supabase/supabase-js";
import pLimit from "p-limit";

// Environment setup
const SUPABASE_URL = process.env.NEXT_PUBLIC_SUPABASE_URL;
const SUPABASE_SERVICE_KEY = process.env.SUPABASE_SERVICE_ROLE_KEY;

if (!SUPABASE_URL || !SUPABASE_SERVICE_KEY) {
  console.error("‚ùå Missing required environment variables:");
  console.error("   NEXT_PUBLIC_SUPABASE_URL");
  console.error("   SUPABASE_SERVICE_ROLE_KEY");
  process.exit(1);
}

const supabase = createClient(SUPABASE_URL, SUPABASE_SERVICE_KEY);

// Configuration
const DATA_DIR = path.join(process.cwd(), "data");
const BATCH_SIZE = 50; // Upsert 50 entities at a time
const CONCURRENCY = 5; // Process 5 batches concurrently

// CLI arguments
const args = process.argv.slice(2);
const isDryRun = args.includes("--dry-run");
const typeFilter = args.find((arg) => arg.startsWith("--type="))?.split("=")[1];
const verbose = args.includes("--verbose") || args.includes("-v");

// Statistics
interface Stats {
  total: number;
  created: number;
  updated: number;
  skipped: number;
  errors: number;
  errorDetails: Array<{ file: string; error: string }>;
}

const stats: Record<string, Stats> = {};

function initStats(type: string) {
  stats[type] = {
    total: 0,
    created: 0,
    updated: 0,
    skipped: 0,
    errors: 0,
    errorDetails: [],
  };
}

/**
 * Recursively read all JSON files in a directory
 */
function readJsonFiles(dir: string): Array<{ path: string; content: any }> {
  const files: Array<{ path: string; content: any }> = [];

  function traverse(currentDir: string) {
    const entries = fs.readdirSync(currentDir, { withFileTypes: true });

    for (const entry of entries) {
      const fullPath = path.join(currentDir, entry.name);

      if (entry.isDirectory()) {
        traverse(fullPath);
      } else if (entry.isFile() && entry.name.endsWith(".json")) {
        try {
          const content = JSON.parse(fs.readFileSync(fullPath, "utf-8"));
          files.push({ path: fullPath, content });
        } catch (error) {
          console.error(`‚ö†Ô∏è  Failed to parse ${fullPath}:`, error);
        }
      }
    }
  }

  traverse(dir);
  return files;
}

/**
 * Determine entity type from file path and content
 */
function determineEntityType(filePath: string, content: any): string {
  // Check content.type first
  if (content.type) {
    return content.type;
  }

  // Infer from directory structure
  if (filePath.includes("/treatments/medications/")) return "medication";
  if (filePath.includes("/treatments/therapy/")) return "therapy";
  if (filePath.includes("/treatments/interventional/")) return "interventional";
  if (filePath.includes("/treatments/investigational/")) return "investigational";
  if (filePath.includes("/treatments/alternative/")) return "alternative";
  if (filePath.includes("/treatments/supplements/")) return "supplement";
  if (filePath.includes("/treatments/")) return "treatment";
  if (filePath.includes("/conditions/")) return "condition";
  if (filePath.includes("/resources/")) return "resource";

  return "unknown";
}

/**
 * Extract category from file path
 */
function extractCategory(filePath: string): string | null {
  const match = filePath.match(/data\/(\w+)\/([\w-]+)\//);
  if (match) {
    return match[2]; // e.g., "medications", "anxiety-fear"
  }
  return null;
}

/**
 * Normalize JSON content to Entity format
 */
function normalizeToEntity(filePath: string, content: any): any {
  const type = determineEntityType(filePath, content);
  const category = extractCategory(filePath);

  // Validate required fields
  if (!content.slug) {
    throw new Error("Missing required field: slug");
  }
  if (!content.name && !content.title) {
    throw new Error("Missing required field: name or title");
  }

  // Build metadata
  const metadata: any = {
    category: category || content.category || content.metadata?.category,
    source: "json-file",
    file_path: path.relative(process.cwd(), filePath),
    last_synced: new Date().toISOString(),
  };

  // Merge existing metadata
  if (content.metadata) {
    Object.assign(metadata, content.metadata);
  }

  // Extract brand names for medications
  if (type === "medication" && content.brand_names) {
    metadata.brand_names = content.brand_names;
  }

  // Extract diagnostic codes for conditions
  if (type === "condition") {
    if (content.dsm5_code) metadata.dsm5_code = content.dsm5_code;
    if (content.icd10_code) metadata.icd10_code = content.icd10_code;
  }

  // Extract pillar for resources
  if (type === "resource" && content.pillar) {
    metadata.pillar = content.pillar;
  }

  return {
    slug: content.slug,
    type,
    title: content.name || content.title,
    description: content.description || content.summary || null,
    content: content, // Store full content in JSONB column
    metadata,
    status: content.status || "active",
  };
}

/**
 * Batch upsert entities to database
 */
async function batchUpsertEntities(entities: any[], type: string): Promise<void> {
  if (isDryRun) {
    console.log(`   [DRY RUN] Would upsert ${entities.length} ${type}s`);
    stats[type].created += entities.length;
    return;
  }

  const { data, error } = await supabase.from("entities").upsert(entities, {
    onConflict: "type,slug",
    ignoreDuplicates: false,
  });

  if (error) {
    console.error(`   ‚ùå Batch upsert failed for ${type}:`, error.message);
    stats[type].errors += entities.length;
    throw error;
  }

  if (verbose) {
    console.log(`   ‚úÖ Upserted ${entities.length} ${type}s`);
  }
}

/**
 * Sync a single content type
 */
async function syncContentType(type: string, directory: string): Promise<void> {
  console.log(`\nüì¶ Syncing ${type}...`);
  initStats(type);

  const dir = path.join(DATA_DIR, directory);
  if (!fs.existsSync(dir)) {
    console.log(`   ‚ö†Ô∏è  Directory not found: ${dir}`);
    return;
  }

  // Read all JSON files
  const files = readJsonFiles(dir);
  stats[type].total = files.length;
  console.log(`   Found ${files.length} files`);

  if (files.length === 0) {
    return;
  }

  // Normalize to entities
  const entities: any[] = [];
  for (const file of files) {
    try {
      const entity = normalizeToEntity(file.path, file.content);
      entities.push(entity);
    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : String(error);
      stats[type].errors++;
      stats[type].errorDetails.push({
        file: path.relative(process.cwd(), file.path),
        error: errorMsg,
      });
      if (verbose) {
        console.error(`   ‚ùå Failed to normalize ${file.path}: ${errorMsg}`);
      }
    }
  }

  if (entities.length === 0) {
    console.log(`   ‚ö†Ô∏è  No valid entities to sync`);
    return;
  }

  // Batch upsert with concurrency control
  const batches: any[][] = [];
  for (let i = 0; i < entities.length; i += BATCH_SIZE) {
    batches.push(entities.slice(i, i + BATCH_SIZE));
  }

  console.log(`   Processing ${batches.length} batches (${BATCH_SIZE} per batch)...`);

  const limit = pLimit(CONCURRENCY);
  const upsertPromises = batches.map((batch, index) =>
    limit(async () => {
      try {
        await batchUpsertEntities(batch, type);
        stats[type].created += batch.length;
        if (!verbose) {
          process.stdout.write(
            `   Progress: ${Math.round(((index + 1) / batches.length) * 100)}%\r`
          );
        }
      } catch (error) {
        stats[type].errors += batch.length;
        console.error(`   ‚ùå Batch ${index + 1} failed:`, error);
      }
    })
  );

  await Promise.all(upsertPromises);

  // Clear progress line
  if (!verbose) {
    process.stdout.write("\n");
  }

  console.log(`   ‚úÖ Completed: ${stats[type].created} synced, ${stats[type].errors} errors`);
}

/**
 * Main sync function
 */
async function main() {
  console.log("üîÑ JSON ‚Üí Database Sync");
  console.log("========================\n");

  if (isDryRun) {
    console.log("üîç DRY RUN MODE - No changes will be made\n");
  }

  console.log(`üìÇ Data directory: ${DATA_DIR}`);
  console.log(`üóÑÔ∏è  Database: ${SUPABASE_URL}\n`);

  const startTime = Date.now();

  // Define sync jobs
  const syncJobs = [
    { type: "treatments", directory: "treatments" },
    { type: "conditions", directory: "conditions" },
    { type: "resources", directory: "resources" },
  ];

  // Filter by type if specified
  const jobs = typeFilter ? syncJobs.filter((job) => job.type === typeFilter) : syncJobs;

  if (jobs.length === 0) {
    console.error(`‚ùå Unknown type: ${typeFilter}`);
    console.error("   Valid types: treatments, conditions, resources");
    process.exit(1);
  }

  // Execute sync jobs sequentially
  for (const job of jobs) {
    await syncContentType(job.type, job.directory);
  }

  const elapsed = Date.now() - startTime;

  // Print summary
  console.log("\n" + "=".repeat(50));
  console.log("üìä SYNC SUMMARY");
  console.log("=".repeat(50) + "\n");

  let totalProcessed = 0;
  let totalErrors = 0;

  for (const [type, stat] of Object.entries(stats)) {
    console.log(`${type.toUpperCase()}:`);
    console.log(`   Total files:  ${stat.total}`);
    console.log(`   Synced:       ${stat.created}`);
    console.log(`   Errors:       ${stat.errors}`);

    if (stat.errorDetails.length > 0 && verbose) {
      console.log(`   Error details:`);
      stat.errorDetails.forEach(({ file, error }) => {
        console.log(`      - ${file}: ${error}`);
      });
    }
    console.log("");

    totalProcessed += stat.created;
    totalErrors += stat.errors;
  }

  console.log(`‚è±Ô∏è  Total time: ${(elapsed / 1000).toFixed(2)}s`);
  console.log(`‚úÖ Successfully synced: ${totalProcessed}`);
  console.log(`‚ùå Errors: ${totalErrors}`);

  if (totalErrors > 0) {
    console.log("\n‚ö†Ô∏è  Some files failed to sync. Run with --verbose for details.");
    process.exit(1);
  }

  console.log("\n‚ú® Sync complete!");
}

// Execute
main().catch((error) => {
  console.error("\n‚ùå Sync failed:", error);
  process.exit(1);
});
